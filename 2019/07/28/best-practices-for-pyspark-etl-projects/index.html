
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://alexioannides.github.io/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://alexioannides.github.io/theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="https://alexioannides.github.io/theme/font-awesome/css/font-awesome.min.css">

    <link href="https://alexioannides.github.io/static/custom.css" rel="stylesheet">

    <link href="https://alexioannides.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Dr Alex Ioannides Atom">


    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-125604661-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

<meta name="author" content="Dr Alex Ioannides" />
<meta name="description" content="I have often lent heavily on Apache Spark and the SparkSQL APIs for operationalising any type of batch data-processing ‘job’, within a production environment where handling fluctuating volumes of data reliably and consistently are on-going business concerns. These batch data-processing jobs may involve nothing more than joining data sources and …" />
<meta name="keywords" content="data-engineering, data-processing, apache-spark, python">

<meta property="og:site_name" content="Dr Alex Ioannides"/>
<meta property="og:title" content="Best Practices for PySpark ETL Projects"/>
<meta property="og:description" content="I have often lent heavily on Apache Spark and the SparkSQL APIs for operationalising any type of batch data-processing ‘job’, within a production environment where handling fluctuating volumes of data reliably and consistently are on-going business concerns. These batch data-processing jobs may involve nothing more than joining data sources and …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://alexioannides.github.io/2019/07/28/best-practices-for-pyspark-etl-projects/"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-07-28 00:00:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://alexioannides.github.io/author/dr-alex-ioannides.html">
<meta property="article:section" content="data-engineering"/>
<meta property="article:tag" content="data-engineering"/>
<meta property="article:tag" content="data-processing"/>
<meta property="article:tag" content="apache-spark"/>
<meta property="article:tag" content="python"/>
<meta property="og:image" content="//avatars1.githubusercontent.com/u/5968486?s=460&v=4">

  <title>Dr Alex Ioannides &ndash; Best Practices for PySpark ETL Projects</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://alexioannides.github.io">
        <img src="//avatars1.githubusercontent.com/u/5968486?s=460&v=4" alt="Dr Alex Ioannides" title="Dr Alex Ioannides">
      </a>
      <h1><a href="https://alexioannides.github.io">Dr Alex Ioannides</a></h1>

<p>machine_learning_engineer - (data)scientist - reformed_quant - habitual_coder</p>
      <nav>
        <ul class="list">
          <li><a href="https://alexioannides.github.io/about-this-blog/">About this&nbsp;Blog</a></li>
          <li><a href="https://alexioannides.github.io/about-me/">About&nbsp;Me</a></li>

        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-github" href="https://github.com/alexioannides" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/alexioannides/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/ioannides_alex" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-soundcloud" href="https://soundcloud.com/user-616657739" target="_blank"><i class="fa fa-soundcloud"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="https://alexioannides.github.io">    Home
</a>

      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>
      <a href="/archives.html">Archives</a>

      <a href="https://alexioannides.github.io/feeds/all.atom.xml">    Atom
</a>

    </nav>

<article class="single">
  <header>
      
    <h1 id="best-practices-for-pyspark-etl-projects">Best Practices for PySpark <span class="caps">ETL</span>&nbsp;Projects</h1>
    <p>
          Posted on Sun 28 July 2019 in <a href="https://alexioannides.github.io/category/data-engineering.html">data-engineering</a>


    </p>
  </header>


  <div>
    <p><img alt="png" src="https://alexioannides.github.io/images/data-engineering/pyspark-etl/etl.png"></p>
<p>I have often lent heavily on Apache Spark and the SparkSQL APIs for operationalising any type of batch data-processing &#8216;job&#8217;, within a production environment where handling fluctuating volumes of data reliably and consistently are on-going business concerns. These batch data-processing jobs may involve nothing more than joining data sources and performing aggregations, or they may apply machine learning models to generate inventory recommendations - regardless of the complexity, this often reduces to defining <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load">Extract, Transform and Load (<span class="caps">ETL</span>)</a> jobs. I&#8217;m a self-proclaimed Pythonista, so I use PySpark for interacting with SparkSQL and for writing and testing all of my <span class="caps">ETL</span>&nbsp;scripts.</p>
<p>This post is designed to be read in parallel with the code in the <a href="https://github.com/AlexIoannides/pyspark-example-project"><code>pyspark-template-project</code> GitHub repository</a>. Together, these constitute what I consider to be a &#8216;best practices&#8217; approach to writing <span class="caps">ETL</span> jobs using Apache Spark and its Python (&#8216;PySpark&#8217;) APIs. These &#8216;best practices&#8217; have been learnt over several years in-the-field, often the result of hindsight and the quest for continuous improvement. I am also grateful to the various contributors to this project for adding their own wisdom to this&nbsp;endeavour. </p>
<p>I aim to addresses the following&nbsp;topics:</p>
<ul>
<li>how to structure <span class="caps">ETL</span> code in such a way that it can be easily tested and&nbsp;debugged;</li>
<li>how to pass configuration parameters to a PySpark&nbsp;job;</li>
<li>how to handle dependencies on other modules and packages;&nbsp;and,</li>
<li>what constitutes a &#8216;meaningful&#8217; test for an <span class="caps">ETL</span>&nbsp;job.</li>
</ul>
<p><strong>Table of&nbsp;Contents</strong></p>
<div class="toc">
<ul>
<li><a href="#pyspark-etl-project-structure">PySpark <span class="caps">ETL</span> Project&nbsp;Structure</a></li>
<li><a href="#the-structure-of-an-etl-job">The Structure of an <span class="caps">ETL</span>&nbsp;Job</a></li>
<li><a href="#passing-configuration-parameters-to-the-etl-job">Passing Configuration Parameters to the <span class="caps">ETL</span>&nbsp;Job</a></li>
<li><a href="#packaging-etl-job-dependencies">Packaging <span class="caps">ETL</span> Job&nbsp;Dependencies</a></li>
<li><a href="#running-the-etl-job">Running the <span class="caps">ETL</span>&nbsp;job</a></li>
<li><a href="#debugging-spark-jobs-using-start_spark">Debugging Spark Jobs Using&nbsp;start_spark</a></li>
<li><a href="#automated-testing">Automated&nbsp;Testing</a></li>
<li><a href="#managing-project-dependencies-using-pipenv">Managing Project Dependencies using Pipenv</a><ul>
<li><a href="#installing-pipenv">Installing&nbsp;Pipenv</a></li>
<li><a href="#installing-this-projects-dependencies">Installing this Projects&#8217;&nbsp;Dependencies</a></li>
<li><a href="#running-python-and-ipython-from-the-projects-virtual-environment">Running Python and IPython from the Project&#8217;s Virtual&nbsp;Environment</a></li>
<li><a href="#pipenv-shells">Pipenv&nbsp;Shells</a></li>
<li><a href="#automatic-loading-of-environment-variables">Automatic Loading of Environment&nbsp;Variables</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
</ul>
</div>
<h2 id="pyspark-etl-project-structure">PySpark <span class="caps">ETL</span> Project&nbsp;Structure</h2>
<p>The basic project structure is as&nbsp;follows:</p>
<div class="highlight"><pre><span></span>root/
<span class="p">|</span>-- configs/
 <span class="p">|</span>   <span class="p">|</span>-- etl_config.json
 <span class="p">|</span>-- dependencies/
 <span class="p">|</span>   <span class="p">|</span>-- logging.py
 <span class="p">|</span>   <span class="p">|</span>-- spark.py
 <span class="p">|</span>-- jobs/
 <span class="p">|</span>   <span class="p">|</span>-- etl_job.py
 <span class="p">|</span>   tests/
 <span class="p">|</span>   <span class="p">|</span>-- test_data/
 <span class="p">|</span>   <span class="p">|</span>-- <span class="p">|</span> -- employees/
 <span class="p">|</span>   <span class="p">|</span>-- <span class="p">|</span> -- employees_report/
 <span class="p">|</span>   <span class="p">|</span>-- test_etl_job.py
 <span class="p">|</span>   Pipfile
 <span class="p">|</span>   Pipfile.lock 
 <span class="p">|</span>   build_dependencies.sh
 <span class="p">|</span>   packages.zip
</pre></div>


<p>The main Python module containing the <span class="caps">ETL</span> job (which will be sent to the Spark cluster), is <code>jobs/etl_job.py</code>. Any external configuration parameters required by <code>etl_job.py</code> are stored in <span class="caps">JSON</span> format in <code>configs/etl_config.json</code>. Additional modules that support this job can be kept in the <code>dependencies</code> folder (more on this later). In the project&#8217;s root we include <code>build_dependencies.sh</code> - a bash script for building these dependencies into a zip-file to be sent to the cluster (<code>packages.zip</code>). Unit test modules are kept in the <code>tests</code> folder and small chunks of representative input and output data, to be use with the tests, are kept in <code>tests/test_data</code> folder.</p>
<h2 id="the-structure-of-an-etl-job">The Structure of an <span class="caps">ETL</span>&nbsp;Job</h2>
<p>In order to facilitate easy debugging and testing, we recommend that the &#8216;Transformation&#8217; step be isolated from the &#8216;Extract&#8217; and &#8216;Load&#8217; steps, into it&#8217;s own function - taking input data arguments in the form of DataFrames and returning the transformed data as a single DataFrame. For example, in the <code>main()</code> job function from <code>jobs/etl_job.py</code> we&nbsp;have,</p>
<div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">extract_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">data_transformed</span> <span class="o">=</span> <span class="n">transform_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="n">load_data</span><span class="p">(</span><span class="n">data_transformed</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>


<p>The code that surrounds the use of the transformation function in the <code>main()</code> job function, is concerned with Extracting the data, passing it to the transformation function and then Loading (or writing) the results to their ultimate destination. Testing is simplified, as mock or test data can be passed to the transformation function and the results explicitly verified, which would not be possible if all of the <span class="caps">ETL</span> code resided in <code>main()</code> and referenced production data sources and&nbsp;destinations.</p>
<p>More generally, transformation functions should be designed to be <a href="https://en.wikipedia.org/wiki/Idempotence">idempotent</a>. This is a technical way of saying&nbsp;that,</p>
<blockquote>
<p>the repeated application of the transformation function to the input data, should have no impact on the fundamental state of output data, until the instance when the input data&nbsp;changes. </p>
</blockquote>
<p>One of the key advantages of idempotent <span class="caps">ETL</span> jobs, is that they can be set to run repeatedly (e.g. by using <code>cron</code> to trigger the <code>spark-submit</code> command on a pre-defined schedule), rather than having to factor-in potential dependencies on other <span class="caps">ETL</span> jobs completing&nbsp;successfully.</p>
<h2 id="passing-configuration-parameters-to-the-etl-job">Passing Configuration Parameters to the <span class="caps">ETL</span>&nbsp;Job</h2>
<p>Although it is possible to pass arguments to <code>etl_job.py</code>, as you would for any generic Python module running as a &#8216;main&#8217; program  - by specifying them after the module&#8217;s filename and then parsing these command line arguments - this can get very complicated, <strong>very quickly</strong>, especially when there are lot of parameters (e.g. credentials for multiple databases, table names, <span class="caps">SQL</span> snippets, etc.). This also makes debugging the code from within a Python interpreter extremely awkward, as you don&#8217;t have access to the command line arguments that would ordinarily be passed to the code, when calling it from the command&nbsp;line.</p>
<p>A much more effective solution is to send Spark a separate file - e.g. using the <code>--files configs/etl_config.json</code> flag with <code>spark-submit</code> - containing the configuration in <span class="caps">JSON</span> format, which can be parsed into a Python dictionary in one line of code with <code>json.loads(config_file_contents)</code>. Testing the code from within a Python interactive console session is also greatly simplified, as all one has to do to access configuration parameters for testing, is to copy and paste the contents of the file -&nbsp;e.g.,</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;{&quot;field&quot;: &quot;value&quot;}&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>


<p>This also has the added bonus that the <span class="caps">ETL</span> job configuration can be explicitly version controlled within the same project structure, avoiding the risk that configuration parameters escape any type of version control - e.g. because they are passed as arguments in bash scripts written by separate teams, whose responsibility is deploying the code, not writing&nbsp;it.  </p>
<p>For the exact details of how the configuration file is located, opened and parsed, please see the <code>start_spark()</code> function in <code>dependencies/spark.py</code> (also discussed in more detail below), which in addition to parsing the configuration file sent to Spark (and returning it as a Python dictionary), also launches the Spark driver program (the application) on the cluster and retrieves the Spark logger at the same&nbsp;time.</p>
<h2 id="packaging-etl-job-dependencies">Packaging <span class="caps">ETL</span> Job&nbsp;Dependencies</h2>
<p>In this project, functions that can be used across different <span class="caps">ETL</span> jobs are kept in a module called <code>dependencies</code> and referenced in specific job modules using, for&nbsp;example,</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dependencies.spark</span> <span class="kn">import</span> <span class="n">start_spark</span>
</pre></div>


<p>This package, together with any additional dependencies referenced within it, must be to copied to each Spark node for all jobs that use <code>dependencies</code> to run. This can be achieved in one of several&nbsp;ways:</p>
<ol>
<li>send all dependencies as a <code>zip</code> archive together with the job, using <code>--py-files</code> with Spark&nbsp;submit;</li>
<li>formally package and upload <code>dependencies</code> to somewhere like the <code>PyPI</code> archive (or a private version) and then run <code>pip3 install dependencies</code> on each node;&nbsp;or,</li>
<li>a combination of manually copying new modules (e.g. <code>dependencies</code>) to the Python path of each node and using <code>pip3 install</code> for additional dependencies (e.g. for <code>requests</code>).</li>
</ol>
<p>Option (1) is by far the easiest and most flexible approach, so we will make use of this. To make this task easier, especially when modules such as <code>dependencies</code> have their own downstream dependencies (e.g. the <code>requests</code> package), we have provided the <code>build_dependencies.sh</code> bash script for automating the production of <code>packages.zip</code>, given a list of dependencies documented in <code>Pipfile</code> and managed by the <a href="https://pipenv.readthedocs.io/en/latest/">Pipenv</a> python application (we discuss the use of Pipenv in greater depth&nbsp;below).</p>
<blockquote>
<p>Note, that dependencies (e.g. NumPy) requiring extensions (e.g. C code) to be compiled locally, will have to be installed manually on each node as part of the node&nbsp;setup.</p>
</blockquote>
<h2 id="running-the-etl-job">Running the <span class="caps">ETL</span>&nbsp;job</h2>
<p>Assuming that the <code>$SPARK_HOME</code> environment variable points to your local Spark installation folder, then the <span class="caps">ETL</span> job can be run from the project&#8217;s root directory using the following command from the&nbsp;terminal,</p>
<div class="highlight"><pre><span></span><span class="nv">$SPARK_HOME</span>/bin/spark-submit <span class="se">\</span>
--master local<span class="o">[</span>*<span class="o">]</span> <span class="se">\</span>
--packages <span class="s1">&#39;com.some-spark-jar.dependency:1.0.0&#39;</span> <span class="se">\</span>
--py-files dependencies.zip <span class="se">\</span>
--files configs/etl_config.json <span class="se">\</span>
jobs/etl_job.py
</pre></div>


<p>Briefly, the options supplied serve the following&nbsp;purposes:</p>
<ul>
<li><code>--master local[*]</code> - the address of the Spark cluster to start the job on. If you have a Spark cluster in operation (either in single-executor mode locally, or something larger in the cloud) and want to send the job there, then modify this with the appropriate Spark <span class="caps">IP</span> - e.g. <code>spark://the-clusters-ip-address:7077</code>;</li>
<li><code>--packages 'com.some-spark-jar.dependency:1.0.0,...'</code> - Maven coordinates for any <span class="caps">JAR</span> dependencies required by the job (e.g. <span class="caps">JDBC</span> driver for connecting to a relational&nbsp;database);</li>
<li><code>--files configs/etl_config.json</code> - the (optional) path to any config file that may be required by the <span class="caps">ETL</span>&nbsp;job;</li>
<li><code>--py-files packages.zip</code> - archive containing Python dependencies (modules) referenced by the job;&nbsp;and,</li>
<li><code>jobs/etl_job.py</code> - the Python module file containing the <span class="caps">ETL</span> job to&nbsp;execute.</li>
</ul>
<p>Full details of all possible options can be found <a href="http://spark.apache.org/docs/latest/submitting-applications.html">here</a>. Note, that we have left some options to be defined within the job (which is actually a Spark application) - e.g. <code>spark.cores.max</code> and <code>spark.executor.memory</code> are defined in the Python script as it is felt that the job should explicitly contain the requests for the required cluster&nbsp;resources.</p>
<h2 id="debugging-spark-jobs-using-start_spark">Debugging Spark Jobs Using <code>start_spark</code></h2>
<p>It is not practical to test and debug Spark jobs by sending them to a cluster using <code>spark-submit</code> and examining stack traces for clues on what went wrong. A more productive workflow is to use an interactive console session (e.g. IPython) or a debugger (e.g. the <code>pdb</code> package in the Python standard library or the Python debugger in Visual Studio Code). In practice, however, it can be hard to test and debug Spark jobs in this way, as they can implicitly rely on arguments that are sent to <code>spark-submit</code>, which are not available in a console or debug&nbsp;session.</p>
<p>We wrote the <code>start_spark</code> function - found in <code>dependencies/spark.py</code> - to facilitate the development of Spark jobs that are aware of the context in which they are being executed - i.e. as <code>spark-submit</code> jobs or within an IPython console, etc. The expected location of the Spark and job configuration parameters required by the job, is contingent on which execution context has been detected. The doscstring for <code>start_spark</code> gives the precise&nbsp;details,</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">start_spark</span><span class="p">(</span><span class="n">app_name</span><span class="o">=</span><span class="s1">&#39;my_spark_app&#39;</span><span class="p">,</span> <span class="n">master</span><span class="o">=</span><span class="s1">&#39;local[*]&#39;</span><span class="p">,</span> <span class="n">jar_packages</span><span class="o">=</span><span class="p">[],</span>
                <span class="n">files</span><span class="o">=</span><span class="p">[],</span> <span class="n">spark_config</span><span class="o">=</span><span class="p">{}):</span>
    <span class="sd">&quot;&quot;&quot;Start Spark session, get Spark logger and load config files.</span>

<span class="sd">    Start a Spark session on the worker node and register the Spark</span>
<span class="sd">    application with the cluster. Note, that only the app_name argument</span>
<span class="sd">    will apply when this is called from a script sent to spark-submit.</span>
<span class="sd">    All other arguments exist solely for testing the script from within</span>
<span class="sd">    an interactive Python console.</span>

<span class="sd">    This function also looks for a file ending in &#39;config.json&#39; that</span>
<span class="sd">    can be sent with the Spark job. If it is found, it is opened,</span>
<span class="sd">    the contents parsed (assuming it contains valid JSON for the ETL job</span>
<span class="sd">    configuration), into a dict of ETL job configuration parameters,</span>
<span class="sd">    which are returned as the last element in the tuple returned by</span>
<span class="sd">    this function. If the file cannot be found then the return tuple</span>
<span class="sd">    only contains the Spark session and Spark logger objects and None</span>
<span class="sd">    for config.</span>

<span class="sd">    The function checks the enclosing environment to see if it is being</span>
<span class="sd">    run from inside an interactive console session or from an</span>
<span class="sd">    environment which has a `DEBUG` environment varibale set (e.g.</span>
<span class="sd">    setting `DEBUG=1` as an environment variable as part of a debug</span>
<span class="sd">    configuration within an IDE such as Visual Studio Code or PyCharm.</span>
<span class="sd">    In this scenario, the function uses all available function arguments</span>
<span class="sd">    to start a PySpark driver from the local PySpark package as opposed</span>
<span class="sd">    to using the spark-submit and Spark cluster defaults. This will also</span>
<span class="sd">    use local module imports, as opposed to those in the zip archive</span>
<span class="sd">    sent to spark via the --py-files flag in spark-submit. </span>

<span class="sd">    Note, if using the local PySpark package on a machine that has the</span>
<span class="sd">    SPARK_HOME environment variable set to a local install of Spark,</span>
<span class="sd">    then the versions will need to match as PySpark appears to pick-up</span>
<span class="sd">    on SPARK_HOME automatically and version conflicts yield errors.</span>

<span class="sd">    :param app_name: Name of Spark app.</span>
<span class="sd">    :param master: Cluster connection details (defaults to local[*].</span>
<span class="sd">    :param jar_packages: List of Spark JAR package names.</span>
<span class="sd">    :param files: List of files to send to Spark cluster (master and</span>
<span class="sd">        workers).</span>
<span class="sd">    :param spark_config: Dictionary of config key-value pairs.</span>
<span class="sd">    :return: A tuple of references to the Spark session, logger and</span>
<span class="sd">        config dict (only if available).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># ...</span>

    <span class="k">return</span> <span class="n">spark_sess</span><span class="p">,</span> <span class="n">spark_logger</span><span class="p">,</span> <span class="n">config_dict</span>
</pre></div>


<p>For example, the following code&nbsp;snippet,</p>
<div class="highlight"><pre><span></span><span class="n">spark</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">config</span> <span class="o">=</span> <span class="n">start_spark</span><span class="p">(</span>
    <span class="n">app_name</span><span class="o">=</span><span class="s1">&#39;my_etl_job&#39;</span><span class="p">,</span>
    <span class="n">jar_packages</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;com.somesparkjar.dependency:1.0.0&#39;</span><span class="p">],</span>
    <span class="n">files</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;configs/etl_config.json&#39;</span><span class="p">])</span>
</pre></div>


<p>Will use the arguments provided to <code>start_spark</code> to setup the Spark job if executed from an interactive console session or debugger, but will look for the same arguments sent via <code>spark-submit</code> if that is how the job has been&nbsp;executed.</p>
<blockquote>
<p>Note, if you are using the local PySpark package - e.g. if running from an interactive console session or debugger - on a machine that also has the <code>SPARK_HOME</code> environment variable set to a local install of Spark, then the two versions will need to match as PySpark appears to pick-up on SPARK_HOME automatically, with version conflicts leading to (unintuitive)&nbsp;errors.</p>
</blockquote>
<h2 id="automated-testing">Automated&nbsp;Testing</h2>
<p>In order to test with Spark, we use the <code>pyspark</code> Python package, which is bundled with the Spark JARs required to programmatically start-up and tear-down a local Spark instance, on a per-test-suite basis (we recommend using the <code>setUp</code> and <code>tearDown</code> methods in <code>unittest.TestCase</code> to do this once per test-suite). Note, that using <code>pyspark</code> to run Spark is an alternative way of developing with Spark as opposed to using the PySpark shell or <code>spark-submit</code>.</p>
<p>Given that we have chosen to structure our <span class="caps">ETL</span> jobs in such a way as to isolate the &#8216;Transformation&#8217; step into its own function (see &#8216;Structure of an <span class="caps">ETL</span> job&#8217; above), we are free to feed it a small slice of &#8216;real-world&#8217; production data that has been persisted locally - e.g. in <code>tests/test_data</code> or some easily accessible network directory - and check it against known results (e.g. computed manually or interactively within a Python interactive console session), as demonstrated in this extract from <code>tests/test_etl_job.py</code>,</p>
<div class="highlight"><pre><span></span><span class="c1"># assemble</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="p">(</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">spark</span>
    <span class="o">.</span><span class="n">read</span>
    <span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">test_data_path</span> <span class="o">+</span> <span class="s1">&#39;employees&#39;</span><span class="p">))</span>

<span class="n">expected_data</span> <span class="o">=</span> <span class="p">(</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">spark</span>
    <span class="o">.</span><span class="n">read</span>
    <span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">test_data_path</span> <span class="o">+</span> <span class="s1">&#39;employees_report&#39;</span><span class="p">))</span>

<span class="n">expected_cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">expected_data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">expected_rows</span> <span class="o">=</span> <span class="n">expected_data</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="n">expected_avg_steps</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">expected_data</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="s1">&#39;steps_to_desk&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;avg_steps_to_desk&#39;</span><span class="p">))</span>
    <span class="o">.</span><span class="n">collect</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">[</span><span class="s1">&#39;avg_steps_to_desk&#39;</span><span class="p">])</span>

<span class="c1"># act</span>
<span class="n">data_transformed</span> <span class="o">=</span> <span class="n">transform_data</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>

<span class="n">cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">expected_data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">rows</span> <span class="o">=</span> <span class="n">expected_data</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="n">avg_steps</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">expected_data</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="s1">&#39;steps_to_desk&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;avg_steps_to_desk&#39;</span><span class="p">))</span>
    <span class="o">.</span><span class="n">collect</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">[</span><span class="s1">&#39;avg_steps_to_desk&#39;</span><span class="p">])</span>

<span class="c1"># assert</span>
<span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">expected_cols</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">expected_rows</span><span class="p">,</span> <span class="n">rows</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">expected_avg_steps</span><span class="p">,</span> <span class="n">avg_steps</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">([</span><span class="n">col</span> <span class="ow">in</span> <span class="n">expected_data</span><span class="o">.</span><span class="n">columns</span>
                 <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">data_transformed</span><span class="o">.</span><span class="n">columns</span><span class="p">])</span>
</pre></div>


<p>To execute the example unit test for this project&nbsp;run,</p>
<div class="highlight"><pre><span></span>pipenv run python -m unittest tests/test_*.py
</pre></div>


<p>If you&#8217;re wondering what the <code>pipenv</code> command is, then read the next&nbsp;section.</p>
<h2 id="managing-project-dependencies-using-pipenv">Managing Project Dependencies using&nbsp;Pipenv</h2>
<p>We use <a href="https://docs.pipenv.org">Pipenv</a> for managing project dependencies and Python environments (i.e. virtual environments). All direct packages dependencies (e.g. NumPy may be used in a User Defined Function), as well as all the packages used during development (e.g. PySpark, flake8 for code linting, IPython for interactive console sessions, etc.), are described in the <code>Pipfile</code>. Their <strong>precise</strong> downstream dependencies are described and frozen in <code>Pipfile.lock</code> (generated automatically by Pipenv, given a&nbsp;Pipfile).</p>
<h3 id="installing-pipenv">Installing&nbsp;Pipenv</h3>
<p>To get started with Pipenv, first of all download it - assuming that there is a global version of Python available on your system and on the <span class="caps">PATH</span>, then this can be achieved by running the following&nbsp;command,</p>
<div class="highlight"><pre><span></span>pip3 install pipenv
</pre></div>


<p>Pipenv is also available to install from many non-Python package managers. For example, on <span class="caps">OS</span> X it can be installed using the <a href="https://brew.sh">Homebrew</a> package manager, with the following terminal&nbsp;command,</p>
<div class="highlight"><pre><span></span>brew install pipenv
</pre></div>


<p>For more information, including advanced configuration options, see the <a href="https://docs.pipenv.org">official Pipenv documentation</a>.</p>
<h3 id="installing-this-projects-dependencies">Installing this Projects&#8217;&nbsp;Dependencies</h3>
<p>Make sure that you&#8217;re in the project&#8217;s root directory (the same one in which the <code>Pipfile</code> resides), and then&nbsp;run,</p>
<div class="highlight"><pre><span></span>pipenv install --dev
</pre></div>


<p>This will install all of the direct project dependencies as well as the development dependencies (the latter a consequence of the <code>--dev</code> flag).</p>
<h3 id="running-python-and-ipython-from-the-projects-virtual-environment">Running Python and IPython from the Project&#8217;s Virtual&nbsp;Environment</h3>
<p>In order to continue development in a Python environment that precisely mimics the one the project was initially developed with, use Pipenv from the command line as&nbsp;follows,</p>
<div class="highlight"><pre><span></span>pipenv run python3
</pre></div>


<p>The <code>python3</code> command could just as well be <code>ipython3</code>, for&nbsp;example,</p>
<div class="highlight"><pre><span></span>pipenv run ipython
</pre></div>


<p>This will fire-up an IPython console session <em>where the default Python 3 kernel includes all of the direct and development project dependencies</em> - this is our&nbsp;preference.</p>
<h3 id="pipenv-shells">Pipenv&nbsp;Shells</h3>
<p>Prepending <code>pipenv</code> to every command you want to run within the context of your Pipenv-managed virtual environment can get very tedious. This can be avoided by entering into a Pipenv-managed&nbsp;shell,</p>
<div class="highlight"><pre><span></span>pipenv shell
</pre></div>


<p>This is equivalent to &#8216;activating&#8217; the virtual environment; any command will now be executed within the virtual environment. Use <code>exit</code> to leave the shell&nbsp;session.</p>
<h3 id="automatic-loading-of-environment-variables">Automatic Loading of Environment&nbsp;Variables</h3>
<p>Pipenv will automatically pick-up and load any environment variables declared in the <code>.env</code> file, located in the package&#8217;s root directory. For example,&nbsp;adding,</p>
<div class="highlight"><pre><span></span><span class="nv">SPARK_HOME</span><span class="o">=</span>applications/spark-2.3.1/bin
<span class="nv">DEBUG</span><span class="o">=</span><span class="m">1</span>
</pre></div>


<p>Will enable access to these variables within any Python program -e.g. via a call to <code>os.environ['SPARK_HOME']</code>. Note, that if any security credentials are placed here, then this file <strong>must</strong> be removed from source control - i.e. add <code>.env</code> to the <code>.gitignore</code> file to prevent potential security&nbsp;risks.</p>
<h2 id="summary">Summary</h2>
<p>The workflow described above, together with the <a href="https://github.com/AlexIoannides/pyspark-example-project">accompanying Python project</a>, represents a stable foundation for writing robust <span class="caps">ETL</span> jobs, regardless of their complexity and regardless of how the jobs are being executed - e.g. via use of <code>cron</code> or more sophisticated workflow automation tools, such as <a href="https://airflow.apache.org">Airflow</a>. I am always interested in collating and integrating more &#8216;best practices&#8217; - if you have any, please submit them <a href="https://github.com/AlexIoannides/pyspark-example-project/issues">here</a>. </p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://alexioannides.github.io/tag/data-engineering.html">data-engineering</a>
      <a href="https://alexioannides.github.io/tag/data-processing.html">data-processing</a>
      <a href="https://alexioannides.github.io/tag/apache-spark.html">apache-spark</a>
      <a href="https://alexioannides.github.io/tag/python.html">python</a>
    </p>
  </div>





<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'alexioannides';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
        Please enable JavaScript to view comments.

</noscript>
<!-- End Disqus -->
</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Dr Alex Ioannides ",
  "url" : "https://alexioannides.github.io",
  "image": "//avatars1.githubusercontent.com/u/5968486?s=460&v=4",
  "description": ""
}
</script>

</body>
</html>