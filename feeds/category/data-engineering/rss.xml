<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Dr Alex Ioannides - data-engineering</title><link>https://alexioannides.github.io/</link><description>machine_learning_engineer - (data)scientist - reformed_quant - habitual_coder</description><lastBuildDate>Sun, 28 Jul 2019 00:00:00 +0100</lastBuildDate><item><title>Best Practices for PySpark ETL Projects</title><link>https://alexioannides.github.io/2019/07/28/best-practices-for-pyspark-etl-projects/</link><description>&lt;p&gt;&lt;img alt="png" src="https://alexioannides.github.io/images/data-engineering/pyspark-etl/etl.png"&gt;&lt;/p&gt;
&lt;p&gt;I have often lent heavily on Apache Spark and the SparkSQL APIs for operationalising any type of batch data-processing &amp;#8216;job&amp;#8217;, within a production environment where handling fluctuating volumes of data reliably and consistently are on-going business concerns. These batch data-processing jobs may involve nothing more than joining data sources and …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Sun, 28 Jul 2019 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2019-07-28:/2019/07/28/best-practices-for-pyspark-etl-projects/</guid><category>data-engineering</category><category>data-engineering</category><category>data-processing</category><category>apache-spark</category><category>python</category></item></channel></rss>