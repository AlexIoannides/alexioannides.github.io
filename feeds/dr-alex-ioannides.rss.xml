<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Dr Alex Ioannides - Dr Alex Ioannides</title><link>https://alexioannides.github.io/</link><description>machine_learning_engineer - (data)scientist - reformed_quant - habitual_coder</description><lastBuildDate>Tue, 01 Dec 2020 00:00:00 +0000</lastBuildDate><item><title>Deploying Python ML Models with Bodywork</title><link>https://alexioannides.github.io/2020/12/01/deploying-python-ml-models-with-bodywork/</link><description>&lt;p&gt;&lt;img alt="bodywork_logo" src="https://alexioannides.github.io/images/machine-learning-engineering/bodywork/bodywork-logo.png"&gt;&lt;/p&gt;
&lt;p&gt;Solutions to Machine Learning (&lt;span class="caps"&gt;ML&lt;/span&gt;) tasks are often developed within Jupyter notebooks. Once a solution is developed you are then faced with an altogether different problem - how to engineer the solution into your product and how to maintain the performance of the solution through time, as new data is&amp;nbsp;generated …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2020-12-01:/2020/12/01/deploying-python-ml-models-with-bodywork/</guid><category>machine-learning-engineering</category><category>python</category><category>machine-learning</category><category>mlops</category><category>kubernetes</category><category>bodywork</category></item><item><title>Best Practices for PySpark ETL Projects</title><link>https://alexioannides.github.io/2019/07/28/best-practices-for-pyspark-etl-projects/</link><description>&lt;p&gt;&lt;img alt="png" src="https://alexioannides.github.io/images/data-engineering/pyspark-etl/etl.png"&gt;&lt;/p&gt;
&lt;p&gt;I have often lent heavily on Apache Spark and the SparkSQL APIs for operationalising any type of batch data-processing &amp;#8216;job&amp;#8217;, within a production environment where handling fluctuating volumes of data reliably and consistently are on-going business concerns. These batch data-processing jobs may involve nothing more than joining data sources and …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Sun, 28 Jul 2019 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2019-07-28:/2019/07/28/best-practices-for-pyspark-etl-projects/</guid><category>data-engineering</category><category>data-engineering</category><category>data-processing</category><category>apache-spark</category><category>python</category></item><item><title>Stochastic Process Calibration using Bayesian Inference &amp; Probabilistic Programs</title><link>https://alexioannides.github.io/2019/01/18/stochastic-process-calibration-using-bayesian-inference-probabilistic-programs/</link><description>&lt;p&gt;&lt;img alt="jpeg" src="https://alexioannides.github.io/images/data_science/bayes_stoch_proc/trading_screen.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Stochastic processes are used extensively throughout quantitative finance - for example, to simulate asset prices in risk models that aim to estimate key risk metrics such as Value-at-Risk (VaR), Expected Shortfall (&lt;span class="caps"&gt;ES&lt;/span&gt;) and Potential Future Exposure (&lt;span class="caps"&gt;PFE&lt;/span&gt;). Estimating the parameters of a stochastic processes - referred to as &amp;#8216;calibration&amp;#8217; in the parlance …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2019-01-18:/2019/01/18/stochastic-process-calibration-using-bayesian-inference-probabilistic-programs/</guid><category>data-science</category><category>probabilistic-programming</category><category>python</category><category>pymc3</category><category>quant-finance</category><category>stochastic-processes</category></item><item><title>Deploying Python ML Models with Flask, Docker and Kubernetes</title><link>https://alexioannides.github.io/2019/01/10/deploying-python-ml-models-with-flask-docker-and-kubernetes/</link><description>&lt;p&gt;&lt;img alt="jpeg" src="https://alexioannides.github.io/images/machine-learning-engineering/k8s-ml-ops/docker+k8s.jpg"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;17th August 2019&lt;/strong&gt; - &lt;em&gt;updated to reflect changes in the Kubernetes &lt;span class="caps"&gt;API&lt;/span&gt; and Seldon&amp;nbsp;Core.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;14th December 2020&lt;/strong&gt; - &lt;em&gt;the work in this post forms the basis of the &lt;a href="https://www.bodyworkml.com"&gt;Bodywork&lt;/a&gt; MLOps tool - read about it &lt;a href="https://alexioannides.github.io/2020/12/01/deploying-python-ml-models-with-bodywork/"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A common pattern for deploying Machine Learning (&lt;span class="caps"&gt;ML&lt;/span&gt;) models into production environments - e.g. &lt;span class="caps"&gt;ML&lt;/span&gt; models …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2019-01-10:/2019/01/10/deploying-python-ml-models-with-flask-docker-and-kubernetes/</guid><category>machine-learning-engineering</category><category>python</category><category>machine-learning</category><category>machine-learning-operations</category><category>kubernetes</category></item><item><title>Bayesian Regression in PYMC3 using MCMC &amp; Variational Inference</title><link>https://alexioannides.github.io/2018/11/07/bayesian-regression-in-pymc3-using-mcmc-variational-inference/</link><description>&lt;p&gt;&lt;img alt="jpeg" src="https://alexioannides.github.io/images/data_science/mcmc_vi_pymc3/pymc3_logo.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Conducting a Bayesian data analysis - e.g. estimating a Bayesian linear regression model - will usually require some form of Probabilistic Programming Language (&lt;span class="caps"&gt;PPL&lt;/span&gt;), unless analytical approaches (e.g. based on conjugate prior models), are appropriate for the task at hand. More often than not, PPLs implement Markov Chain Monte Carlo …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2018-11-07:/2018/11/07/bayesian-regression-in-pymc3-using-mcmc-variational-inference/</guid><category>data-science</category><category>machine-learning</category><category>probabilistic-programming</category><category>python</category><category>pymc3</category></item><item><title>Machine Learning Pipelines for R</title><link>https://alexioannides.github.io/2017/05/08/machine-learning-pipelines-for-r/</link><description>&lt;p&gt;&lt;img alt="pipes" src="https://alexioannides.github.io/images/r/pipeliner/pipelines1.png" title="Pipelines!"&gt;&lt;/p&gt;
&lt;p&gt;Building machine learning and statistical models often requires pre- and post-transformation of the input and/or response variables, prior to training (or fitting) the models. For example, a model may require training on the logarithm of the response and input variables. As a consequence, fitting and then generating predictions from …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Mon, 08 May 2017 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2017-05-08:/2017/05/08/machine-learning-pipelines-for-r/</guid><category>r</category><category>machine-learning</category><category>data-processing</category></item><item><title>elasticsearchr - a Lightweight Elasticsearch Client for R</title><link>https://alexioannides.github.io/2016/11/28/elasticsearchr-a-lightweight-elasticsearch-client-for-r/</link><description>&lt;p&gt;&lt;img alt="elasticsearchr" src="https://alexioannides.github.io/images/r/elasticsearchr/elasticsearchr2.png" title="Elasticsearchr"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.elastic.co/products/elasticsearch" title="Elasticsearch"&gt;Elasticsearch&lt;/a&gt; is a distributed &lt;a href="https://en.wikipedia.org/wiki/NoSQL" title="What is NoSQL?"&gt;NoSQL&lt;/a&gt; document store search-engine and &lt;a href="https://www.elastic.co/blog/elasticsearch-as-a-column-store" title="Elasticsearch as a Column Store"&gt;column-oriented database&lt;/a&gt;, whose &lt;strong&gt;fast&lt;/strong&gt; (near real-time) reads and powerful aggregation engine make it an excellent choice as an &amp;#8216;analytics database&amp;#8217; for R&amp;amp;D, production-use or both. Installation is simple, it ships with default settings that allow it to work effectively out-of-the-box …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Mon, 28 Nov 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2016-11-28:/2016/11/28/elasticsearchr-a-lightweight-elasticsearch-client-for-r/</guid><category>r</category><category>data-processing</category><category>data-stores</category></item><item><title>Asynchronous and Distributed Programming in R with the Future Package</title><link>https://alexioannides.github.io/2016/11/02/asynchronous-and-distributed-programming-in-r-with-the-future-package/</link><description>&lt;p&gt;&lt;img alt="Future!" src="https://alexioannides.github.io/images/r/future/the_future.jpg" title="the_future"&gt;&lt;/p&gt;
&lt;p&gt;Every now and again someone comes along and writes an R package that I consider to be a &amp;#8216;game changer&amp;#8217; for the language and it&amp;#8217;s application to Data Science. For example, I consider &lt;a href="https://github.com/hadley/dplyr" title="dplyr on GitHub"&gt;dplyr&lt;/a&gt; one such package as it has made data munging/manipulation &lt;em&gt;that&lt;/em&gt; more intuitive and more …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Wed, 02 Nov 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2016-11-02:/2016/11/02/asynchronous-and-distributed-programming-in-r-with-the-future-package/</guid><category>r</category><category>data-processing</category><category>high-performance-computing</category></item><item><title>An R Function for Generating Authenticated URLs to Private Web Sites Hosted on AWS S3</title><link>https://alexioannides.github.io/2016/09/19/an-r-function-for-generating-authenticated-urls-to-private-web-sites-hosted-on-aws-s3/</link><description>&lt;p&gt;&lt;img alt="crypto" src="https://alexioannides.files.wordpress.com/2016/08/hmac.png" title="HMAC"&gt;&lt;/p&gt;
&lt;p&gt;Quite often I want to share simple (static) web pages with other colleagues or clients. For example, I may have written a report using &lt;a href="http://rmarkdown.rstudio.com" title="R Markdown @ R Studio"&gt;R Markdown&lt;/a&gt; and rendered it to &lt;span class="caps"&gt;HTML&lt;/span&gt;. &lt;span class="caps"&gt;AWS&lt;/span&gt; S3 can easily host such a simple web page (e.g. see &lt;a href="http://docs.aws.amazon.com/gettingstarted/latest/swh/website-hosting-intro.html" title="AWS S3 Static Web Page"&gt;here&lt;/a&gt;), but it cannot, however, offer …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Mon, 19 Sep 2016 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2016-09-19:/2016/09/19/an-r-function-for-generating-authenticated-urls-to-private-web-sites-hosted-on-aws-s3/</guid><category>r</category><category>AWS</category></item><item><title>Building a Data Science Platform for R&amp;D, Part 4 - Apache Zeppelin &amp; Scala Notebooks</title><link>https://alexioannides.github.io/2016/08/29/building-a-data-science-platform-for-rd-part-4-apache-zeppelin-scala-notebooks/</link><description>&lt;p&gt;&lt;img alt="zeppelin" src="https://alexioannides.github.io/images/data_science/data_science_platform_pt4/zeppelin.png" title="Apache Zeppelin"&gt;&lt;/p&gt;
&lt;p&gt;Parts &lt;a href="https://alexioannides.github.io/2016/08/16/building-a-data-science-platform-for-rd-part-1-setting-up-aws/" title="Part 1"&gt;one&lt;/a&gt;, &lt;a href="https://alexioannides.github.io/2016/08/18/building-a-data-science-platform-for-rd-part-2-deploying-spark-on-aws-using-flintrock/" title="Part 2"&gt;two&lt;/a&gt; and &lt;a href="https://alexioannides.github.io/2016/08/22/building-a-data-science-platform-for-rd-part-3-r-r-studio-server-sparkr-sparklyr/" title="Part 3"&gt;three&lt;/a&gt; of this series of posts have taken us from creating an account on &lt;span class="caps"&gt;AWS&lt;/span&gt; to loading and interacting with data in Spark via R and R Studio. My vision of a Data Science platform for R&amp;amp;D is nearly complete - the only outstanding component is …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Mon, 29 Aug 2016 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2016-08-29:/2016/08/29/building-a-data-science-platform-for-rd-part-4-apache-zeppelin-scala-notebooks/</guid><category>data-science</category><category>AWS</category><category>data-processing</category></item><item><title>Building a Data Science Platform for R&amp;D, Part 3 - R, R Studio Server, SparkR &amp; Sparklyr</title><link>https://alexioannides.github.io/2016/08/22/building-a-data-science-platform-for-rd-part-3-r-r-studio-server-sparkr-sparklyr/</link><description>&lt;p&gt;&lt;img alt="Alt" src="https://alexioannides.github.io/images/data_science/data_science_platform_pt3/sparklyr.png" title="Command Line R"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://alexioannides.github.io/2016/08/16/building-a-data-science-platform-for-rd-part-1-setting-up-aws/" title="Part 1"&gt;Part 1&lt;/a&gt; and &lt;a href="https://alexioannides.github.io/2016/08/18/building-a-data-science-platform-for-rd-part-2-deploying-spark-on-aws-using-flintrock/" title="Part 2"&gt;Part 2&lt;/a&gt; of this series dealt with setting up &lt;span class="caps"&gt;AWS&lt;/span&gt;, loading data into S3, deploying a Spark cluster and using it to access our data. In this part we will deploy R and R Studio Server to our Spark cluster&amp;#8217;s master node and use it to …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Mon, 22 Aug 2016 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2016-08-22:/2016/08/22/building-a-data-science-platform-for-rd-part-3-r-r-studio-server-sparkr-sparklyr/</guid><category>data-science</category><category>AWS</category><category>data-processing</category><category>apache-spark</category></item><item><title>Building a Data Science Platform for R&amp;D, Part 2 - Deploying Spark on AWS using Flintrock</title><link>https://alexioannides.github.io/2016/08/18/building-a-data-science-platform-for-rd-part-2-deploying-spark-on-aws-using-flintrock/</link><description>&lt;p&gt;&lt;img alt="Alt" src="https://alexioannides.github.io/images/data_science/data_science_platform_pt2/spark.png" title="spark"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://alexioannides.github.io/2016/08/16/building-a-data-science-platform-for-rd-part-1-setting-up-aws/" title="PartOne"&gt;Part 1&lt;/a&gt; in this series of blog posts describes how to setup &lt;span class="caps"&gt;AWS&lt;/span&gt; with some basic security and then load data into S3. This post walks-through the process of setting up a Spark cluster on &lt;span class="caps"&gt;AWS&lt;/span&gt; and accessing our S3 data from within&amp;nbsp;Spark.&lt;/p&gt;
&lt;p&gt;A key part of my vision …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Thu, 18 Aug 2016 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2016-08-18:/2016/08/18/building-a-data-science-platform-for-rd-part-2-deploying-spark-on-aws-using-flintrock/</guid><category>data-science</category><category>AWS</category><category>data-processing</category><category>apache-spark</category></item><item><title>Building a Data Science Platform for R&amp;D, Part 1 - Setting-Up AWS</title><link>https://alexioannides.github.io/2016/08/16/building-a-data-science-platform-for-rd-part-1-setting-up-aws/</link><description>&lt;p&gt;&lt;img alt="Alt" src="https://alexioannides.github.io/images/data_science/data_science_platform_pt1/aws.png" title="AWS"&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s my vision: I get into the office and switch-on my laptop; then I start-up my &lt;a href="https://spark.apache.org"&gt;Spark&lt;/a&gt; cluster; I interact with it via &lt;a href="https://www.rstudio.com"&gt;RStudio&lt;/a&gt; to exploring a new dataset a client uploaded overnight; after getting a handle on what I want to do with it, I prototype an &lt;span class="caps"&gt;ETL …&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr Alex Ioannides</dc:creator><pubDate>Tue, 16 Aug 2016 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:alexioannides.github.io,2016-08-16:/2016/08/16/building-a-data-science-platform-for-rd-part-1-setting-up-aws/</guid><category>data-science</category><category>AWS</category><category>data-processing</category></item></channel></rss>