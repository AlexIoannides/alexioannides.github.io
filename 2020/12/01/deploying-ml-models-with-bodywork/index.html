
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://alexioannides.github.io/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://alexioannides.github.io/theme/pygments/monokai.min.css">


  <link rel="stylesheet" type="text/css" href="https://alexioannides.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://alexioannides.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://alexioannides.github.io/theme/font-awesome/css/solid.css">

    <link href="https://alexioannides.github.io/static/custom.css" rel="stylesheet">

    <link href="https://alexioannides.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Dr Alex Ioannides Atom">


    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-125604661-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

<meta name="author" content="Dr Alex Ioannides" />
<meta name="description" content="Tags: python, machine-learning, mlops, kubernetes, bodywork Solutions to ML problems are usually first developed in Jupyter notebooks. We are then faced with an altogether different problem - how to engineer these notebook solutions into your products and systems and continue to maintain their performance through time, after new data is generated …" />
<meta name="keywords" content="">


<meta property="og:site_name" content="Dr Alex Ioannides"/>
<meta property="og:title" content="Deploying ML Models with Bodywork"/>
<meta property="og:description" content="Tags: python, machine-learning, mlops, kubernetes, bodywork Solutions to ML problems are usually first developed in Jupyter notebooks. We are then faced with an altogether different problem - how to engineer these notebook solutions into your products and systems and continue to maintain their performance through time, after new data is generated …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://alexioannides.github.io/2020/12/01/deploying-ml-models-with-bodywork/"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-12-01 00:00:00+00:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://alexioannides.github.io/author/dr-alex-ioannides.html">
<meta property="article:section" content="machine-learning-engineering"/>
<meta property="og:image" content="//avatars1.githubusercontent.com/u/5968486?s=460&v=4">

  <title>Dr Alex Ioannides &ndash; Deploying ML Models with Bodywork</title>

</head>
<body class="light-theme">
  <aside>
    <div>
      <a href="https://alexioannides.github.io">
        <img src="//avatars1.githubusercontent.com/u/5968486?s=460&v=4" alt="Dr Alex Ioannides" title="Dr Alex Ioannides">
      </a>

      <h1>
        <a href="https://alexioannides.github.io">Dr Alex Ioannides</a>
      </h1>

<p>machine_learning_engineer - (data)scientist - reformed_quant - habitual_coder</p>

      <nav>
        <ul class="list">


              <li>
                <a target="_self"
                   href="https://alexioannides.github.io/about-me/">
                  About&nbsp;Me
                </a>
              </li>
              <li>
                <a target="_self"
                   href="https://alexioannides.github.io/about-this-blog/">
                  About this&nbsp;Blog
                </a>
              </li>

            <li>
              <a target="_self" href="https://alexioannides.com/notes-and-demos/" >Study Notes & Demos</a>
            </li>
        </ul>
      </nav>

      <ul class="social">
          <li>
            <a  class="sc-github" href="https://github.com/alexioannides" target="_blank">
              <i class="fab fa-github"></i>
            </a>
          </li>
          <li>
            <a  class="sc-linkedin" href="https://www.linkedin.com/in/alexioannides/" target="_blank">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
          <li>
            <a  class="sc-twitter" href="https://twitter.com/ioannides_alex" target="_blank">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
          <li>
            <a  class="sc-soundcloud" href="https://soundcloud.com/user-616657739" target="_blank">
              <i class="fab fa-soundcloud"></i>
            </a>
          </li>
      </ul>
    </div>

  </aside>
  <main>

    <nav>
      <a href="https://alexioannides.github.io">Home</a>

      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>
      <a href="/archives.html">Archives</a>

      <a href="https://alexioannides.github.io/feeds/all.atom.xml">Atom</a>

    </nav>

<article class="single">
  <header>
      
    <h1 id="deploying-ml-models-with-bodywork">Deploying <span class="caps">ML</span> Models with&nbsp;Bodywork</h1>
    <p>
      Posted on Tue 01 December 2020 in <a href="https://alexioannides.github.io/category/machine-learning-engineering.html">machine-learning-engineering</a>

    </p>
  </header>


  <div>
    <p>Tags: python, machine-learning, mlops, kubernetes,&nbsp;bodywork</p>
<p><img alt="bodywork_logo" src="https://alexioannides.github.io/images/machine-learning-engineering/bodywork/bodywork-cli.png"></p>
<p>Solutions to <span class="caps">ML</span> problems are usually first developed in Jupyter notebooks. We are then faced with an altogether different problem - how to engineer these notebook solutions into your products and systems and continue to maintain their performance through time, after new data is&nbsp;generated.</p>
<p><strong>Table of&nbsp;Contents</strong></p>
<div class="toc">
<ul>
<li><a href="#what-is-this-tutorial-going-to-teach-me">What is this Tutorial Going to Teach&nbsp;Me?</a></li>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#why-is-mlops-getting-so-much-attention">Why is MLOps Getting so Much&nbsp;Attention?</a></li>
<li><a href="#ml-deployment-with-bodywork"><span class="caps">ML</span> Deployment with&nbsp;Bodywork</a></li>
</ul>
</li>
<li><a href="#before-we-start">Before we&nbsp;Start</a></li>
<li><a href="#the-ml-task">The <span class="caps">ML</span>&nbsp;Task</a></li>
<li><a href="#a-continuous-training-pipeline">A Continuous Training&nbsp;Pipeline</a></li>
<li><a href="#configuring-the-training-stage">Configuring the Training&nbsp;Stage</a></li>
<li><a href="#configuring-the-prediction-service">Configuring the Prediction&nbsp;Service</a></li>
<li><a href="#configuring-the-pipeline">Configuring the&nbsp;Pipeline</a></li>
<li><a href="#deploying-the-pipeline">Deploying the&nbsp;Pipeline</a></li>
<li><a href="#testing-the-api">Testing the <span class="caps">API</span></a></li>
<li><a href="#scheduling-the-pipeline">Scheduling the&nbsp;Pipeline</a></li>
<li><a href="#cleaning-up">Cleaning&nbsp;Up</a></li>
</ul>
</div>
<h2 id="what-is-this-tutorial-going-to-teach-me">What is this Tutorial Going to Teach&nbsp;Me?</h2>
<ul>
<li>How to re-engineer a <span class="caps">ML</span> solution from a Jupyter notebook into a production-ready Python&nbsp;modules.</li>
<li>How to develop a two-stage <span class="caps">ML</span> pipeline that trains a model and then creates a prediction service to exposes it via a <span class="caps">REST</span> <span class="caps">API</span>.</li>
<li>How to deploy the pipeline to <a href="https://kubernetes.io/">Kubernetes</a> using <a href="https://github.com/">GitHub</a> and <a href="https://bodywork.readthedocs.io/en/latest/">Bodywork</a>.</li>
<li>How to configure the pipeline to run on a schedule, so the model is periodically re-trained and re-deployed without the intervention of an <span class="caps">ML</span>&nbsp;engineer.</li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>I’ve written at length on the subject of getting machine learning into production - an area that now falls under Machine Learning Operations (MLOps). MLOps has become a hot topic - take my <a href="https://alexioannides.github.io/2019/01/10/deploying-python-ml-models-with-flask-docker-and-kubernetes/">blog post</a> on <em>Deploying Python <span class="caps">ML</span> Models with Flask, Docker and Kubernetes</em>, which is accessed by hundreds of <span class="caps">ML</span> practitioners every month; or the fact that Thoughtwork’s <a href="https://www.thoughtworks.com/insights/articles/intelligent-enterprise-series-cd4ml">essay</a> on <em>Continuous Delivery for <span class="caps">ML</span></em> has become an essential reference for all <span class="caps">ML</span> engineers, together with Google’s <a href="https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html">paper</a> on the <em>Hidden Technical Debt in <span class="caps">ML</span> Systems</em>; and MLOps even has its own entry on <a href="https://en.wikipedia.org/wiki/MLOps">Wikipedia</a>.</p>
<h3 id="why-is-mlops-getting-so-much-attention">Why is MLOps Getting so Much&nbsp;Attention?</h3>
<p>In my opinion, this is because we are at a point where a significant number of organisations have now overcome their data ingestion and engineering problems. They are able to provide their data scientists with the data required to solve business problems using <span class="caps">ML</span>, only to find that, as Thoughtworks put&nbsp;it,</p>
<blockquote>
<p>“<em>Getting machine learning applications into production is hard</em>”</p>
</blockquote>
<p>To tackle some of the core complexities of MLOps, many <span class="caps">ML</span> engineering teams have settled on approaches that are based-upon deploying containerised models, usually as RESTful prediction services, to some type of cloud platform. Kubernetes is especially useful for this as I have <a href="https://alexioannides.github.io/2019/01/10/deploying-python-ml-models-with-flask-docker-and-kubernetes/">written about before</a>.</p>
<h3 id="ml-deployment-with-bodywork"><span class="caps">ML</span> Deployment with&nbsp;Bodywork</h3>
<p>Running <span class="caps">ML</span> code in containers has become a common pattern to guarantee reproducibility between what has been developed and what is deployed in production&nbsp;environments.</p>
<p>Most <span class="caps">ML</span> engineers do not, however, have the time to develop the skills and expertise required to deliver and deploy containerised <span class="caps">ML</span> systems into production environments. This requires an understanding of how to build container images, how to push build artefacts to image repositories and how to configure a container orchestration platform to use these, to execute batch jobs and deploy&nbsp;services.</p>
<p>Developing and maintaining these deployment pipelines is time-consuming. If there are multiple projects - each requiring re-training and re-deployment - then the management of these pipelines will quickly become a large&nbsp;burden.</p>
<p>This is where Bodywork steps-in - it will deliver your project&#8217;s Python modules directly from your Git repository into Docker containers and manage their deployment to a Kubernetes cluster. In other words, Bodywork automates the repetitive tasks that most <span class="caps">ML</span> engineers think of as <a href="https://en.wikipedia.org/wiki/DevOps">DevOps</a>, allowing them to focus their time on what they do best - i.e., engineering solutions to <span class="caps">ML</span>&nbsp;tasks.</p>
<p>This post serves as a short tutorial on how to use Bodywork to productionise a common pipeline pattern (train-and-deploy), and it will refer to files within a Bodywork project hosted on GitHub - see <a href="https://github.com/bodywork-ml/bodywork-ml-pipeline-project">bodywork-ml-pipeline-project</a>.</p>
<p><img alt="bodywork_logo" src="https://alexioannides.github.io/images/machine-learning-engineering/bodywork/ml-pipeline.png"></p>
<h2 id="before-we-start">Before we&nbsp;Start</h2>
<p>If you want to run the examples you will need to <a href="https://bodywork.readthedocs.io/en/latest/installation/">install Bodywork</a> on your machine and setup access to Kubernetes (see this <a href="https://bodywork.readthedocs.io/en/latest/kubernetes/#quickstart">Kubernetes Quickstart Guide</a> for help here). I recommend that you find five minutes to read about the <a href="https://bodywork.readthedocs.io/en/latest/key_concepts/">key concepts</a> that Bodywork is built upon, before beginning to work-through the examples&nbsp;below.</p>
<h2 id="the-ml-task">The <span class="caps">ML</span>&nbsp;Task</h2>
<p>The <span class="caps">ML</span> problem we have chosen to use for this example, is the classification of iris plants into one of their three sub-species, given their physical dimensions. It uses the infamous <a href="https://scikit-learn.org/stable/datasets/index.html#iris-dataset">iris plants dataset</a> and is an example of a multi-class classification&nbsp;task.</p>
<p>The Jupyter notebook titled <a href="https://github.com/bodywork-ml/bodywork-ml-pipeline-project/blob/master/notebooks/ml_prototype_work.ipynb">ml_prototype_work.ipynb</a>, documents the trivial <span class="caps">ML</span> workflow used to arrive at a solution to this task. It trains a Decision Tree classifier and persists the trained model to cloud storage (an <span class="caps">AWS</span> bucket). Take five minutes to read through&nbsp;it.</p>
<h2 id="a-continuous-training-pipeline">A Continuous Training&nbsp;Pipeline</h2>
<p>The two stage train-and-deploy pipeline is packaged as a <a href="https://github.com/bodywork-ml/bodywork-ml-pipeline-project">GitHub repository</a>, and is structured as&nbsp;follows,</p>
<div class="highlight"><pre><span></span><code>root/
 |-- notebooks/
     |-- ml_prototype_work.ipynb
 |-- pipeline/
     |-- train_model.py
     |-- serve_model.py
 |-- bodywork.yaml
</code></pre></div>

<p>All the configuration for this deployment is held within <code>bodywork.yaml</code>, whose contents are reproduced&nbsp;below.</p>
<div class="highlight"><pre><span></span><code><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1.1&quot;</span>

<span class="nt">pipeline</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bodywork-ml-pipeline-project</span>
<span class="w">  </span><span class="nt">docker_image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bodyworkml/bodywork-core:latest</span>
<span class="w">  </span><span class="nt">DAG</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">stage_1_train_model &gt;&gt; stage_2_scoring_service</span>

<span class="nt">stages</span><span class="p">:</span>
<span class="w">  </span><span class="nt">stage_1_train_model</span><span class="p">:</span>
<span class="w">    </span><span class="nt">executable_module_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pipeline/train_model.py</span>
<span class="w">    </span><span class="nt">requirements</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">boto3==1.21.14</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">joblib==1.1.0</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pandas==1.4.1</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">scikit-learn==1.0.2</span>
<span class="w">    </span><span class="nt">cpu_request</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span>
<span class="w">    </span><span class="nt">memory_request_mb</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">batch</span><span class="p">:</span>
<span class="w">      </span><span class="nt">max_completion_time_seconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
<span class="w">      </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>

<span class="w">  </span><span class="nt">stage_2_scoring_service</span><span class="p">:</span>
<span class="w">    </span><span class="nt">executable_module_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pipeline/serve_model.py</span>
<span class="w">    </span><span class="nt">requirements</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">flask==2.1.2</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">joblib==1.1.0</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">numpy==1.22.3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">scikit-learn==1.0.2</span>
<span class="w">    </span><span class="nt">cpu_request</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.25</span>
<span class="w">    </span><span class="nt">memory_request_mb</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">service</span><span class="p">:</span>
<span class="w">      </span><span class="nt">max_startup_time_seconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
<span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5000</span>
<span class="w">      </span><span class="nt">ingress</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="nt">logging</span><span class="p">:</span>
<span class="w">  </span><span class="nt">log_level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">INFO</span>
</code></pre></div>

<p>The remainder of this tutorial is concerned with explaining how the configuration within <code>bodywork.yaml</code> is used to deploy the pipeline, as defined within the <code>train_model.py</code> and <code>serve_model.py</code> Python&nbsp;modules.</p>
<h2 id="configuring-the-training-stage">Configuring the Training&nbsp;Stage</h2>
<p>The <code>stages.stage_1_train_model.executable_module_path</code> points to the executable Python module - <code>train_model.py</code> - that defines what will happen when the <code>stage_1_train_model</code> (batch) stage is executed, within a pre-built <a href="https://hub.docker.com/repository/docker/bodyworkml/bodywork-core">Bodywork container</a>. This module contains the code required&nbsp;to:</p>
<ol>
<li>download data from an <span class="caps">AWS</span> S3&nbsp;bucket;</li>
<li>pre-process the data (e.g. extract labels for supervised&nbsp;learning);</li>
<li>train the model and compute performance metrics;&nbsp;and,</li>
<li>persist the model to the same <span class="caps">AWS</span> S3 bucket that contains the original&nbsp;data.</li>
</ol>
<p>It can be summarised&nbsp;as,</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>

<span class="c1"># other imports</span>
<span class="c1"># ...</span>

<span class="n">DATA_URL</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;http://bodywork-ml-pipeline-project.s3.eu-west-2.amazonaws.com&#39;</span>
            <span class="s1">&#39;/data/iris_classification_data.csv&#39;</span><span class="p">)</span>

<span class="c1"># other constants</span>
<span class="c1"># ...</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Main script to be executed.&quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">download_dataset</span><span class="p">(</span><span class="n">DATA_URL</span><span class="p">)</span>
    <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">pre_process_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">trained_model</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">persist_model</span><span class="p">(</span><span class="n">trained_model</span><span class="p">)</span>


<span class="c1"># other functions definitions used in main()</span>
<span class="c1"># ...</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>

<p>We recommend that you spend five minutes familiarising yourself with the full contents of <a href="https://github.com/bodywork-ml/bodywork-ml-pipeline-project/blob/master/pipeline/train_model.py">train_model.py</a>. When Bodywork runs the stage, it will do so in exactly the same way as if you were to&nbsp;run,</p>
<div class="highlight"><pre><span></span><code>$ python train_model.py
</code></pre></div>

<p>And so everything defined in <code>main()</code> will be&nbsp;executed.</p>
<p>The <code>stages.stage_1_train_model.requirements</code> parameter in the <code>bodywork.yaml</code> file lists the 3rd party Python packages that will be Pip-installed on the pre-built Bodywork container, as required to run the <code>train_model.py</code> module. In this example we&nbsp;have,</p>
<div class="highlight"><pre><span></span><code>boto3==1.21.14
joblib==1.1.0
pandas==1.4.1
scikit-learn==1.0.2
</code></pre></div>

<ul>
<li><code>boto3</code> - for interacting with <span class="caps">AWS</span>;</li>
<li><code>joblib</code> - for persisting&nbsp;models;</li>
<li><code>pandas</code> - for manipulating the raw data;&nbsp;and,</li>
<li><code>scikit-learn</code> - for training the&nbsp;model.</li>
</ul>
<p>Finally, the remaining parameters in <code>stages.stage_1_train_model</code> section of <code>bodywork.yaml</code> allow us to configure the remaining key parameters for the&nbsp;stage,</p>
<div class="highlight"><pre><span></span><code><span class="nt">stage_1_train_model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">executable_module_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">stage_1_train_model/train_model.py</span>
<span class="w">  </span><span class="nt">requirements</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">boto3==1.21.14</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">joblib==1.1.0</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pandas==1.4.1</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">scikit-learn==1.0.2</span>
<span class="w">  </span><span class="nt">cpu_request</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span>
<span class="w">  </span><span class="nt">memory_request_mb</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">  </span><span class="nt">batch</span><span class="p">:</span>
<span class="w">    </span><span class="nt">max_completion_time_seconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
<span class="w">    </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</code></pre></div>

<p>From which it is clear to see that we have specified that this stage is a batch stage (as opposed to a service-deployment), together with an estimate of the <span class="caps">CPU</span> and memory resources to request from the Kubernetes cluster, how long to wait and how many times to retry,&nbsp;etc.</p>
<h2 id="configuring-the-prediction-service">Configuring the Prediction&nbsp;Service</h2>
<p>The <code>stages.stage_2_scoring_service.executable_module_path</code> parameter points to the executable Python module - <code>serve_model.py</code> - that defines what will happen when the <code>stage_2_scoring_service</code> (service) stage is executed, within a pre-built Bodywork container. This module contains the code required&nbsp;to:</p>
<ol>
<li>load the model trained in <code>stage_1_train_model</code> and persisted to cloud storage;&nbsp;and,</li>
<li>start a Flask service to score instances (or rows) of data, sent as <span class="caps">JSON</span> to the <span class="caps">API</span>&nbsp;endpoint.</li>
</ol>
<p>We chose to develop the prediction service using <a href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a>, but this is <strong>not</strong> a requirement in any way and you are free to use any frameworks you like - e.g., <a href="https://fastapi.tiangolo.com">FastAPI</a>.</p>
<p>The contents of <code>serve_model.py</code> defines the <span class="caps">REST</span> <span class="caps">API</span> server and can be summarised&nbsp;as,</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>

<span class="c1"># other imports</span>
<span class="c1"># ...</span>

<span class="n">MODEL_URL</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;http://bodywork-ml-pipeline-project.s3.eu-west-2.amazonaws.com/models&#39;</span>
             <span class="s1">&#39;/iris_tree_classifier.joblib&#39;</span><span class="p">)</span>

<span class="c1"># other constants</span>
<span class="c1"># ...</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">Flask</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s1">&#39;/iris/v1/score&#39;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;POST&#39;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">score</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Response</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iris species classification API endpoint&quot;&quot;&quot;</span>
    <span class="n">request_data</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">json</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">make_features_from_request_data</span><span class="p">(</span><span class="n">request_data</span><span class="p">)</span>
    <span class="n">model_output</span> <span class="o">=</span> <span class="n">model_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">response_data</span> <span class="o">=</span> <span class="n">jsonify</span><span class="p">({</span><span class="o">**</span><span class="n">model_output</span><span class="p">,</span> <span class="s1">&#39;model_info&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">model</span><span class="p">)})</span>
    <span class="k">return</span> <span class="n">make_response</span><span class="p">(</span><span class="n">response_data</span><span class="p">)</span>


<span class="c1"># other functions definitions used in score() and below</span>
<span class="c1"># ...</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">MODEL_URL</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;loaded model=</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;starting API server&#39;</span><span class="p">)</span>
    <span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;0.0.0.0&#39;</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</code></pre></div>

<p>We recommend that you spend five minutes familiarising yourself with the full contents of <a href="https://github.com/bodywork-ml/bodywork-ml-pipeline-project/blob/master/pipeline/serve_model.py">serve_model.py</a>. When Bodywork runs the stage, it will start the server defined by <code>app</code> and expose the <code>/iris/v1/score</code> route that is being handled by <code>score()</code>. Note, that this process has no scheduled end and the stage will be kept up-and-running until it is re-deployed or <a href="https://bodywork.readthedocs.io/en/latest/user_guide/#deleting-services">deleted</a>.</p>
<p>The <code>stages.stage_2_scoring_service.requirements</code> parameter in the <code>bodywork.yaml</code> file lists the 3rd party Python packages that will be Pip-installed on the pre-built Bodywork container, as required to run the <code>serve_model.py</code> module. In this example we&nbsp;have,</p>
<div class="highlight"><pre><span></span><code>boto3==1.21.14
joblib==1.1.0
pandas==1.4.1
scikit-learn==1.0.2
</code></pre></div>

<ul>
<li><code>Flask</code> - the framework upon which the <span class="caps">REST</span> <span class="caps">API</span> server is&nbsp;built;</li>
<li><code>joblib</code> - for loading the persisted&nbsp;model;</li>
<li><code>numpy</code> <span class="amp">&amp;</span> <code>scikit-learn</code> - for working with the <span class="caps">ML</span>&nbsp;model.</li>
</ul>
<p>Finally, the remaining parameters in <code>stages.stage_2_scoring_service</code> section of <code>bodywork.yaml</code> allow us to configure the remaining key parameters for the&nbsp;stage,</p>
<div class="highlight"><pre><span></span><code><span class="nt">stage_2_scoring_service</span><span class="p">:</span>
<span class="w">  </span><span class="nt">executable_module_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">stage_2_scoring_service/serve_model.py</span>
<span class="w">  </span><span class="nt">requirements</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">flask==2.1.2</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">joblib==1.1.0</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">numpy==1.22.3</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">scikit-learn==1.0.2</span>
<span class="w">  </span><span class="nt">cpu_request</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.25</span>
<span class="w">  </span><span class="nt">memory_request_mb</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">  </span><span class="nt">service</span><span class="p">:</span>
<span class="w">    </span><span class="nt">max_startup_time_seconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30</span>
<span class="w">    </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5000</span>
<span class="w">    </span><span class="nt">ingress</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>

<p>From which it is clear to see that we have specified that this stage will create a service (as opposed to run a batch job), together with an estimate of the <span class="caps">CPU</span> and memory resources to request from the Kubernetes cluster, how long to wait for the service to start-up and be &#8216;ready&#8217;, which port to expose, to create a path to the service from an externally-facing ingress controller (if present in the cluster), and how many instances (or replicas) of the server should be created to stand-behind the&nbsp;cluster-service.</p>
<h2 id="configuring-the-pipeline">Configuring the&nbsp;Pipeline</h2>
<p>The <code>project</code> section of <code>bodywork.yaml</code> contains the configuration for the&nbsp;pipeline,</p>
<div class="highlight"><pre><span></span><code><span class="nt">pipeline</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bodywork-ml-pipeline-project</span>
<span class="w">  </span><span class="nt">docker_image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bodyworkml/bodywork-core:latest</span>
<span class="w">  </span><span class="nt">DAG</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">stage_1_train_model &gt;&gt; stage_2_scoring_service</span>
</code></pre></div>

<p>The most important element is the specification of the workflow <span class="caps">DAG</span>, which in this instance is simple and will instruct the Bodywork workflow-controller to first run the training stage, and then (if successful) create the prediction&nbsp;service.</p>
<h2 id="deploying-the-pipeline">Deploying the&nbsp;Pipeline</h2>
<p>To deploy the pipeline and create the prediction service, use the following&nbsp;command,</p>
<div class="highlight"><pre><span></span><code>$ bw create deployment &quot;https://github.com/bodywork-ml/bodywork-ml-pipeline-project&quot;
</code></pre></div>

<p>Which will run the pipeline defined in the default branch of the project&#8217;s remote Git repository (e.g., <code>master</code>), and stream the logs to stdout -&nbsp;e.g,</p>
<div class="highlight"><pre><span></span><code>========================================== deploying master branch from https://github.com/bodywork-ml/bodywork-ml-pipeline-project ===========================================
[02/21/22 14:50:59] INFO     Creating k8s namespace = bodywork-ml-pipeline-project                                                                                             
[02/21/22 14:51:00] INFO     Creating k8s service account = bodywork-stage                                                                                                     
[02/21/22 14:51:00] INFO     Attempting to execute DAG step = [stage_1_train_model]                                                                                            
[02/21/22 14:51:00] INFO     Creating k8s job for stage = stage-1-train-model  
...
</code></pre></div>

<h2 id="testing-the-api">Testing the <span class="caps">API</span></h2>
<p>The details of any serviced associated with the pipeline, can be retrieved&nbsp;using,</p>
<div class="highlight"><pre><span></span><code>$ bw get deployment &quot;bodywork-ml-pipeline-project&quot; &quot;stage-2-scoring-service&quot;

┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Field                ┃ Value                                                                         ┃
┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ name                 │ stage-2-scoring-service                                                       │
│ namespace            │ bodywork-ml-pipeline-project                                                  │
│ service_exposed      │ True                                                                          │
│ service_url          │ http://stage-2-scoring-service.bodywork-ml-pipeline-project.svc.cluster.local │
│ service_port         │ 5000                                                                          │
│ available_replicas   │ 2                                                                             │
│ unavailable_replicas │ 0                                                                             │
│ git_url              │ https://github.com/bodywork-ml/bodywork-ml-pipeline-project                   │
│ git_branch           │ master                                                                        │
│ git_commit_hash      │ e9df4b4                                                                       │
│ has_ingress          │ True                                                                          │
│ ingress_route        │ /bodywork-ml-pipeline-project/stage-2-scoring-service                         │
└──────────────────────┴───────────────────────────────────────────────────────────────────────────────┘
</code></pre></div>

<p>Services are accessible via the public internet if you have <a href="https://bodywork.readthedocs.io/en/latest/kubernetes/#installing-nginx">installed an ingress controller</a> within your cluster, and the <code>stages.STAGE_NAME.service.ingress</code> <a href="#service-deployment-stages">configuration parameter</a> is set to <code>true</code>. If you are using Kubernetes via Minikube and our <a href="https://bodywork.readthedocs.io/en/latest/kubernetes/#quickstart">Kuberentes Quickstart</a> guide, then this will have been enabled for you. Otherwise, services will only be accessible via <span class="caps">HTTP</span> from <strong>within</strong> the cluster, via the <code>service_url</code>.</p>
<p>Assuming that you are setup to access services from outside the cluster, then you can test the endpoint&nbsp;using,</p>
<div class="highlight"><pre><span></span><code>$ curl http://YOUR_CLUSTERS_EXTERNAL_IP/bodywork-ml-pipeline-project/stage-2-scoring-service/iris/v1/score \
    --request POST \
    --header &quot;Content-Type: application/json&quot; \
    --data &#39;{&quot;sepal_length&quot;: 5.1, &quot;sepal_width&quot;: 3.5, &quot;petal_length&quot;: 1.4, &quot;petal_width&quot;: 0.2}&#39;
</code></pre></div>

<p>See <a href="https://bodywork.readthedocs.io/en/latest/kubernetes/#accessing-services">here</a> for instructions on how to retrieve <code>YOUR_CLUSTERS_EXTERNAL_IP</code> if you are using Minikube, otherwise refer to the instructions <a href="https://bodywork.readthedocs.io/en/latest/kubernetes/#connecting-to-the-cluster">here</a>. This request ought to&nbsp;return,</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;species_prediction&quot;</span><span class="p">:</span><span class="s2">&quot;setosa&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;probabilities&quot;</span><span class="p">:</span><span class="s2">&quot;setosa=1.0|versicolor=0.0|virginica=0.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;model_info&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;DecisionTreeClassifier(class_weight=&#39;balanced&#39;, random_state=42)&quot;</span>
<span class="p">}</span>
</code></pre></div>

<p>According to how the payload has been defined in the <code>stage_2_scoring_service/serve_model.py</code> module.</p>
<h2 id="scheduling-the-pipeline">Scheduling the&nbsp;Pipeline</h2>
<p>If you&#8217;re happy with the results of this test deployment, you can then schedule the pipeline to run on the cluster, on a schedule. For example, to setup the the workflow to run every day at midnight, use the following&nbsp;command,</p>
<div class="highlight"><pre><span></span><code>$ bw create cronjob &quot;https://github.com/bodywork-ml/bodywork-ml-pipeline-project&quot; \
    --name &quot;daily&quot; \
    --schedule &quot;0 * * * *&quot; \
    --retries 2
</code></pre></div>

<p>Each scheduled pipeline execution will attempt to run the pipeline - i.e., retraining the model and updating the prediction service - as defined by the state of this repository&#8217;s default branch (<code>master</code>), at the time of execution. To change the branch used for deployment, use the <code>--branch</code> option.</p>
<p>To get the execution history for this cronjob&nbsp;use,</p>
<div class="highlight"><pre><span></span><code>$ bw get cronjob &quot;daily&quot; --history
</code></pre></div>

<p>Which should return output along the lines&nbsp;of,</p>
<div class="highlight"><pre><span></span><code>           run ID = daily-1645446900
┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Field           ┃ Value                     ┃
┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ start_time      │ 2022-02-21 12:35:06+00:00 │
│ completion_time │ 2022-02-21 12:39:32+01:03 │
│ active          │ False                     │
│ succeeded       │ True                      │
│ failed          │ False                     │
└─────────────────┴───────────────────────────┘
</code></pre></div>

<p>Then to stream the logs from any given cronjob run (e.g. to debug and/or monitor for errors),&nbsp;use,</p>
<div class="highlight"><pre><span></span><code>$ bw get cronjobs daily --logs &quot;hourly-1645446900&quot;
</code></pre></div>

<h2 id="cleaning-up">Cleaning&nbsp;Up</h2>
<p>To tear-down the prediction service created by the pipeline you can&nbsp;use,</p>
<div class="highlight"><pre><span></span><code>$ bw delete deployment &quot;bodywork-ml-pipeline-project&quot;
</code></pre></div>
  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>





<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'alexioannides';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Please enable JavaScript to view comments.
</noscript>
<!-- End Disqus -->
</article>

    <footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Dr Alex Ioannides ",
  "url" : "https://alexioannides.github.io",
  "image": "//avatars1.githubusercontent.com/u/5968486?s=460&v=4",
  "description": ""
}
</script>


</body>
</html>